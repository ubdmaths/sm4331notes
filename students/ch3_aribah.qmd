# Random Sampling

When experimenting, the data collected can be represented or modeled as a set of **random variables** that describe the observed values.

We can model this by assuming $X = {X_1,\dots, X_n}$ sampled from a population whose pdf or pmf is $f_x(x)$

We can write this as $${X_1,\dots, X_n} \sim f_x(x)$$

::: {#nte-sampling}
**Key Assumption** ðŸ“Œ: The distribution $f_x$ is imposed on data as a model.
:::

## Independent and Identically Distributed (IID) Samples

The samples are assumed to be:

-   Independent: No relationship between observations.
-   Identical Distribution: Each sample follows the same probability distribution.

As a result, the **joint probability density function (pdf)** can be written as:

$$ f_X(X_1, \dots, X_n) = \prod_{i=1}^{n} f(X_i) $$

If from a parametric family:

$$ f_X(X_1, \dots, X_n |\theta) = \prod_{i=1}^{n} f(X_i|\theta) $$

# Statistics and Sampling Distributions

::: {#def-statistics}
**Definition 1(Statistics)**

A function of the sample $T_n = T(X_1, \dots, X_n)$ that does not depend on unknown parameters, **only on observables**.
:::

Two common examples:

-   the sample mean

$$
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i 
$$

-   the sample variance (unbiased)

$$
S^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2
$$

These are used to estimate the true population mean $\mu$ and variance $\sigma^2$ respectively.

## Sampling Distribution

Since a **statistic** is a random variable, it follows a probability **distribution**. Understanding this distribution allows us to assess how well the statistic estimates the true population parameter.

Key considerations that we need to include:

-   What is the **expected value** of the statistic?
-   How **close** is it to the population parameter $\theta$?
-   What is the probability that the statistic falls within a small range of $\theta$, i.e. $|T_n - \theta| < \epsilon$?

This analysis is fundamental in inferential statistics, as it helps determine the reliability and accuracy of estimators.

### Properties of Sample Mean and Variance estimators

::: {#thm-properties}
**Theorem 2**

Let $X_1,\dots, X_n$ be an independent sample from a population with mean $\mu$ and variance $\sigma^2 < \infty$. Assuming that their mgf exists and is equal to $M_X(t)$, then:

i.  $E(\bar{X}) = \mu$
ii. $Var(\bar{X}) = \frac{\sigma^2}{n}$
iii. $E(S^2) = \sigma^2$
iv. $M_\bar{X}(t) = [M_X(\frac{t}{n})]^n$
:::

### Normal random samples

::: {thm-randomsample}
**Theorem 4**

Let $\{X_1, \dots, X_n\}$ be an independent sample from $\text{N}(\mu, \sigma^2)$. Then,

i.  $\bar{X} \sim \text{N}(\mu, \frac{\sigma^2}{n})$
ii. $(n-1)\frac{S^2}{\sigma^2} \sim \chi^2_{n-1}$
iii. $\bar{X}$ and $S^2$ are independent rvs
iv. $\frac{\sqrt{\bar{X}-\mu}}{S} \sim \text{t}_{n-1}$
:::

# Ï‡Â²-Distribution

::: {def-chisquare}
**Definition 5 (**$\chi^2$-distribution)

Let $Z_1, \dots, Z_k \overset{\text{iid}}{\sim}\text{N}(0, 1)$, i.e. each $Z_i$ has pdf $f(Z_i) = (2\pi)^{-1/2}e^{-Z_i^2/2}$ for $i = 1, \dots, k$. Then,

$$
X = {Z_1}^2 + \dots + {Z_k}^2 = \sum_{i=1}^{k} {Z_i}^2
$$ follows a $\chi^2$-distribution with $k \in \mathbb{N}$ degrees of freedom. We can write this as $X \sim \chi_{k}^{2}$.
:::

The pdf of a $\chi_{k}^{2}$-distribution is

$$
f(x) = \frac{1}{2^\frac{k}{2}\Gamma(k/2)}x^{\frac{k}{2}-1}e^\frac{-x}{2}
$$

-   Support: $0 \leq x < \infty$
-   Parameters: $k$ (d.f)
-   $E(X) = k$
-   $Var(X) = 2k$

::: {lem-chisquare}
**Lemma 6**

If $X_1 \sim \chi^2_{k_1}$ and $X_2 \sim \chi^2_{k_2}$, and $X_1 \perp\!\!\!\perp X_2$, then $X_1 + X_2 \sim \chi^2_{k_1 + k_2}$.
:::

## Probability table for the Ï‡Â²-Distribution

**Key Idea**ðŸ’¡ : Why use probability tables for $\chi^2$?

The probability $P(X \leq x)$ for a $\chi^2$-distributed random variable **cannot be calculated in closed form** because its cumulative distribution function (CDF) involves an incomplete gamma function. Instead, we use numerical methods or probability tables.

If $X \sim \chi^2_k$ , the probability of obtaining a value $x$ or smaller is:

$$
P(X \leq x) = \int^x_0 f_X(\tilde{x}) d\tilde{x}
$$

where $f_X(x)$ is the probability density function (pdf) of the $\chi^2$ distribution. This integral **cannot be solved exactly** in general, so we rely on **approximations and probability tables**.
