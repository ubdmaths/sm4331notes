{
  "hash": "93409c2d44350bd48a9bf7aacbc0f883",
  "result": {
    "engine": "knitr",
    "markdown": "\\usepackage{blkarray}\n\n\\newcommand{\\bzero}{{\\mathbf 0}}\n\\newcommand{\\bone}{{\\mathbf 1}}\n\\newcommand{\\ba}{{\\mathbf a}}\n\\newcommand{\\bb}{{\\mathbf b}}\n\\newcommand{\\bc}{{\\mathbf c}}\n\\newcommand{\\bd}{{\\mathbf d}}\n\\newcommand{\\be}{{\\mathbf e}}\n\\newcommand{\\bff}{{\\mathbf f}}\n\\newcommand{\\bg}{{\\mathbf g}}\n\\newcommand{\\bh}{{\\mathbf h}}\n\\newcommand{\\bi}{{\\mathbf i}}\n\\newcommand{\\bj}{{\\mathbf j}}\n\\newcommand{\\bk}{{\\mathbf k}}\n\\newcommand{\\bl}{{\\mathbf l}}\n\\newcommand{\\bmm}{{\\mathbf m}}\n\\newcommand{\\bn}{{\\mathbf n}}\n\\newcommand{\\bo}{{\\mathbf o}}\n\\newcommand{\\bp}{{\\mathbf p}}\n\\newcommand{\\bq}{{\\mathbf q}}\n\\newcommand{\\br}{{\\mathbf r}}\n\\newcommand{\\bs}{{\\mathbf s}}\n\\newcommand{\\bt}{{\\mathbf t}}\n\\newcommand{\\bu}{{\\mathbf u}}\n\\newcommand{\\bv}{{\\mathbf v}}\n\\newcommand{\\bw}{{\\mathbf w}}\n\\newcommand{\\bx}{{\\mathbf x}}\n\\newcommand{\\by}{{\\mathbf y}}\n\\newcommand{\\bz}{{\\mathbf z}}\n\\newcommand{\\bA}{{\\mathbf A}}\n\\newcommand{\\bB}{{\\mathbf B}}\n\\newcommand{\\bC}{{\\mathbf C}}\n\\newcommand{\\bD}{{\\mathbf D}}\n\\newcommand{\\bE}{{\\mathbf E}}\n\\newcommand{\\bF}{{\\mathbf F}}\n\\newcommand{\\bG}{{\\mathbf G}}\n\\newcommand{\\bH}{{\\mathbf H}}\n\\newcommand{\\bI}{{\\mathbf I}}\n\\newcommand{\\bJ}{{\\mathbf J}}\n\\newcommand{\\bK}{{\\mathbf K}}\n\\newcommand{\\bL}{{\\mathbf L}}\n\\newcommand{\\bM}{{\\mathbf M}}\n\\newcommand{\\bN}{{\\mathbf N}}\n\\newcommand{\\bO}{{\\mathbf O}}\n\\newcommand{\\bP}{{\\mathbf P}}\n\\newcommand{\\bQ}{{\\mathbf Q}}\n\\newcommand{\\bR}{{\\mathbf R}}\n\\newcommand{\\bS}{{\\mathbf S}}\n\\newcommand{\\bT}{{\\mathbf T}}\n\\newcommand{\\bU}{{\\mathbf U}}\n\\newcommand{\\bV}{{\\mathbf V}}\n\\newcommand{\\bW}{{\\mathbf W}}\n\\newcommand{\\bX}{{\\mathbf X}}\n\\newcommand{\\bY}{{\\mathbf Y}}\n\\newcommand{\\bZ}{{\\mathbf Z}}\n\\newcommand{\\balpha}{{\\boldsymbol\\alpha}}\n\\newcommand{\\bbeta}{{\\boldsymbol\\beta}}\n\\newcommand{\\bgamma}{{\\boldsymbol\\gamma}}\n\\newcommand{\\bdelta}{{\\boldsymbol\\delta}}\n\\newcommand{\\bepsilon}{{\\boldsymbol\\epsilon}}\n\\newcommand{\\bvarepsilon}{{\\boldsymbol\\varepsilon}}\n\\newcommand{\\bzeta}{{\\boldsymbol\\zeta}}\n\\newcommand{\\bfeta}{{\\boldsymbol\\eta}}\n\\newcommand{\\boldeta}{{\\boldsymbol\\eta}}\n\\newcommand{\\btheta}{{\\boldsymbol\\theta}}\n\\newcommand{\\bvartheta}{{\\boldsymbol\\vartheta}}\n\\newcommand{\\biota}{{\\boldsymbol\\iota}}\n\\newcommand{\\bkappa}{{\\boldsymbol\\kappa}}\n\\newcommand{\\blambda}{{\\boldsymbol\\lambda}}\n\\newcommand{\\bmu}{{\\boldsymbol\\mu}}\n\\newcommand{\\bnu}{{\\boldsymbol\\nu}}\n\\newcommand{\\bxi}{{\\boldsymbol\\xi}}\n\\newcommand{\\bpi}{{\\boldsymbol\\pi}}\n\\newcommand{\\bvarpi}{{\\boldsymbol\\varpi}}\n\\newcommand{\\brho}{{\\boldsymbol\\rho}}\n\\newcommand{\\bvarrho}{{\\boldsymbol\\varrho}}\n\\newcommand{\\bsigma}{{\\boldsymbol\\sigma}}\n\\newcommand{\\bvarsigma}{{\\boldsymbol\\varsigma}}\n\\newcommand{\\btau}{{\\boldsymbol\\tau}}\n\\newcommand{\\bupsilon}{{\\boldsymbol\\upsilon}}\n\\newcommand{\\bphi}{{\\boldsymbol\\phi}}\n\\newcommand{\\bvarphi}{{\\boldsymbol\\varphi}}\n\\newcommand{\\bchi}{{\\boldsymbol\\chi}}\n\\newcommand{\\bpsi}{{\\boldsymbol\\psi}}\n\\newcommand{\\bomega}{{\\boldsymbol\\omega}}\n\\newcommand{\\bGamma}{{\\boldsymbol\\Gamma}}\n\\newcommand{\\bDelta}{{\\boldsymbol\\Delta}}\n\\newcommand{\\bTheta}{{\\boldsymbol\\Theta}}\n\\newcommand{\\bLambda}{{\\boldsymbol\\Lambda}}\n\\newcommand{\\bXi}{{\\boldsymbol\\Xi}}\n\\newcommand{\\bPi}{{\\boldsymbol\\Pi}}\n\\newcommand{\\bSigma}{{\\boldsymbol\\Sigma}}\n\\newcommand{\\bUpsilon}{{\\boldsymbol\\Upsilon}}\n\\newcommand{\\bPhi}{{\\boldsymbol\\Phi}}\n\\newcommand{\\bPsi}{{\\boldsymbol\\Psi}}\n\\newcommand{\\bOmega}{{\\boldsymbol\\Omega}}\n\\DeclareMathOperator{\\diag}{diag}\n\\DeclareMathOperator{\\Prob}{P}\n\\DeclareMathOperator{\\E}{E}\n\\DeclareMathOperator{\\Var}{Var}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\Corr}{Corr}\n\\DeclareMathOperator{\\sd}{sd}\n\\DeclareMathOperator{\\se}{se}\n\\DeclareMathOperator{\\N}{N}\n\\DeclareMathOperator{\\Bin}{Bin}\n\\DeclareMathOperator{\\Bern}{Bern}\n\\DeclareMathOperator{\\Dir}{Dir}\n\\DeclareMathOperator{\\Wis}{Wis}\n\\DeclareMathOperator{\\logit}{logit}\n\\DeclareMathOperator{\\expit}{expit}\n\\DeclareMathOperator{\\Mult}{Mult}\n\\DeclareMathOperator{\\Cat}{Cat}\n\\DeclareMathOperator{\\Pois}{Poi}\n\\DeclareMathOperator{\\Geom}{Geom}\n\\DeclareMathOperator{\\NBin}{NBin}\n\\DeclareMathOperator{\\Exp}{Exp}\n\\DeclareMathOperator{\\Betadist}{Beta}\n\\DeclareMathOperator{\\Hypergeom}{Hypergeom}\n\\DeclareMathOperator{\\Cauchy}{Cauchy}\n\\DeclareMathOperator{\\hCauchy}{half-Cauchy}\n\\DeclareMathOperator{\\LKJ}{LKJ}\n\\DeclareMathOperator{\\Unif}{Unif}\n\\DeclareMathOperator{\\KL}{KL}\n\\DeclareMathOperator{\\ind}{\\mathds{1}}\n\\newcommand{\\iid}{\\,\\overset{\\text{iid}}{\\sim}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\DeclareMathOperator{\\Lik}{L}\n\\DeclareMathOperator{\\argmax}{argmax}\n\\DeclareMathOperator{\\vecc}{vec}\n\\DeclareMathOperator{\\dd}{d}\n\\newcommand{\\dint}{\\dd\\hspace{0.5pt}\\!}\n\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\bbZ}{\\mathbb{Z}}\n\\newcommand{\\bbC}{\\mathbb{C}}\n\\newcommand{\\bbS}{\\mathbb{S}}\n\\newcommand{\\bbH}{\\mathbb{H}}\n\\newcommand{\\bbP}{\\mathbb{P}}\n\n\\newcommand{\\cA}{{\\mathcal A}}\n\\newcommand{\\cB}{{\\mathcal B}}\n\\newcommand{\\cC}{{\\mathcal C}}\n\\newcommand{\\cD}{{\\mathcal D}}\n\\newcommand{\\cE}{{\\mathcal E}}\n\\newcommand{\\cF}{{\\mathcal F}}\n\\newcommand{\\cG}{{\\mathcal G}}\n\\newcommand{\\cH}{{\\mathcal H}}\n\\newcommand{\\cI}{{\\mathcal I}}\n\\newcommand{\\cJ}{{\\mathcal J}}\n\\newcommand{\\cK}{{\\mathcal K}}\n\\newcommand{\\cL}{{\\mathcal L}}\n\\newcommand{\\cM}{{\\mathcal M}}\n\\newcommand{\\cN}{{\\mathcal N}}\n\\newcommand{\\cO}{{\\mathcal O}}\n\\newcommand{\\cP}{{\\mathcal P}}\n\\newcommand{\\cQ}{{\\mathcal Q}}\n\\newcommand{\\cR}{{\\mathcal R}}\n\\newcommand{\\cS}{{\\mathcal S}}\n\\newcommand{\\cT}{{\\mathcal T}}\n\\newcommand{\\cU}{{\\mathcal U}}\n\\newcommand{\\cV}{{\\mathcal V}}\n\\newcommand{\\cW}{{\\mathcal W}}\n\\newcommand{\\cX}{{\\mathcal X}}\n\\newcommand{\\cY}{{\\mathcal Y}}\n\\newcommand{\\cZ}{{\\mathcal Z}}\n\n\\newcommand{\\myoverbrace}[3][gray]{{\\color{#1}\\overbrace{\\color{black}#2}^{#3}}}\n\\newcommand{\\myunderbrace}[3][gray]{{\\color{#1}\\underbrace{\\color{black}#2}_{#3}}}\n\n\\renewcommand{\\Pr}{\\operatorname{P}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Hypothesis testing\n\n\n## A fuzzy and cute example\n\nOn a farm there are 499 white bunnies, and 1 brown bunny.\nOne of the bunnies ravaged through the carrot farm, leaving the farmer furious.\n\n![](figures/white_brown_bunny.png){#fig-whitebrownbunny}\n\n\n::: {.callout-note}\n### Question\n\nCan we say that we know, or reasonably believe with confidence, that it was a white bunny that caused the problem? What's your proof?^[Example adapted from Schoeman, F. (1987). Statistical vs. direct evidence. No√ªs, 179-198.]\n:::\n\n\nAssume colour difference is not associated with behavioural differences in rabbits.\nIf we believe that a white rabbit indeed was at fault, the error rate is 1/500 = 0.2%.\n\nSuppose there was a witness that claimed the brown rabbit did it. The witness performed a colour identification test, reporting the right colour 95% of the time.\nGiven the evidence, the probability that a brown rabbit was at fault is\n\n\\begin{align*}\n\\Pr(B\\mid E)\n&= \\frac{\\Pr(E \\mid B) \\Pr(B)}{\\Pr(E \\mid B) \\Pr(B) + \\Pr(E \\mid B^c) \\Pr(B^c)} \\\\\n&= \\frac{0.95 \\times 0.002}{0.95 \\times 0.002 + 0.05 \\times 0.998} \\\\\n&\\approx 3.6\\%\n\\end{align*}\n\ngiving an error rate of 96.4\\%!\n  \n## Fisherian view\n\nThe $p$-value is interpreted as a *continuous measure of evidence* \\underline{against}  some null hypothesis--there is no point at which the results become 'significant'.\n\n::: {.callout-caution}\n### Remark\nStatistical evidence differs from direct evidence (e.g. having CCTV recording in the house).\nWe may **never know** what exactly happened. \nThe best we can do is to base decisions based on the *likelihood of the evidence* materialising.\n:::\n\n![Credits: <https://xkcd.com/892/>.](https://imgs.xkcd.com/comics/null_hypothesis.png){width=50% fig-align=\"center\" #fig-xkcd-hyp}\n\n\n\n## Uniformity of $p$-values\n\n::: {.callout-note}\n### Question\nSince $p(\\bX)$ is a statistic, it is a rv. What is its distribution?\n:::\n\n::: {#thm-uniformity-pvalues}\n### Uniformity of $p$-values\nIf $\\theta_0$ is a point null hypothesis for the parameter of continuous $\\bX$, then a correctly calculated $p$-value $p_T(\\bX)$ based on any test statistic $W$, is such that \n$$\np_T(\\bX) \\sim \\Unif(0,1)\n$$\nin repeated sampling under $H_0$.\n:::\n\n\n\n::: {.proof}\nThis is a consequence of the \\emph{probability integral transform}: \nSuppose that a continuous rv $T$ has cdf $F_T(t), \\forall t$. Then the rv $Y=F_T(T)\\sim\\Unif(0,1)$ because: \\vspace{-1.2em}\n$$\nF_Y(y)=\\Pr( \\myoverbrace{F_T(T)}{Y}\\leq y) = \\Pr\\big(T \\leq F^{-1}_T(y)\\big) = F_T\\left(F^{-1}_T(y) \\right) = y,\n$$\nwhich is the cdf of a $\\Unif(0,1)$ distribution.\n\n\n\nNow for any data $\\bx$,\n$$\np_T(\\bx) =  \\Pr\\!{}_{\\theta_0}\\left(T(\\bX) \\geq T(\\bx) \\right) = 1 - F\\big( T(\\bx) \\big),\n$$\nwhere $F$ is the cdf (under $H_0$) of $T(\\bX)$. Hence, $p_T(\\bx)=1-Y$ where $Y\\sim\\Unif(0,1)$ by the probability integral transform. But clearly if $Y\\sim\\Unif(0,1)$, then so is $1-Y$.\n:::    \n\nThis result is useful especially for *checking the validity* of a complicated $p$-value calculation:\n\n1. Simulate several new data sets from the null distribution.\n\n2. For each simulated data set, apply the $p$-value calculation.\n\n3. Assess the collection of resulting $p$-values--do they seem to be uniformly distributed?\n\nSuppose we are testing $H_0:\\mu=0$ on a random sample of $X_1,\\dots,X_n$ assumed to be normally distributed with mean $\\mu$ and variance $\\sigma^2=4.3^2$.\nLet's do this experiment:\n\n1. Draw $X_1,\\dots,X_{10}\\iid\\N(0,4.3^2)$ (the distribution under $H_0$)\n2. Compute the $p$-value $p(\\bx)$ based on the simulated data\n3. Repeat 1--2 a total of $B=100000$ times to get $p_1,\\dots,p_B \\in (0,1)$\n\n\nPlotting a histogram of the simulated $p$-values yields:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10\nsigma <- 4.3\nB <- 100000\nres <- rep(NA, B)\nfor (i in 1:B) {\n  x <- rnorm(n, sd = sigma)\n  res[i] <- 2 * (pnorm((sqrt(n) * abs(mean(x)) / sigma), lower.tail = FALSE))\n}\nggplot() + \n  geom_histogram(aes(res, ..density..), breaks = seq(0, 1, by = 0.05),\n                 col = \"white\") +\n  geom_hline(yintercept = 1, linetype = \"dashed\") +\n  scale_y_continuous(breaks = seq(0, 1, by = 0.25)) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1)) +\n  labs(x = \"p\", y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](chapter6_files/figure-epub/fig-uniformpvalues-1.png){#fig-uniformpvalues}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy is this? Assume that $H_0$ is true. In the Neyman-Pearson approach, $\\alpha$ is the rate of false positives, i.e. the rate at which the null hypothesis is rejected given that $H_0$ is true. This rate is  fixed. \nOn the other hand, $p=p(\\bX)$ is a random variable.\n\nFor any value $\\alpha$, the null is rejected when the observed $p < \\alpha$. \nThis happens, by definition, with probability $\\alpha$!\nThe only way that this happens is when the $p$-value comes from a uniform  distribution, since $\\Pr(U \\leq u) = u$. \nI.e., under the null\n\n- $p$ has a 5% chance of being less than $\\alpha=0.05$;\n- $p$ has a 10% chance of being less than $\\alpha=0.1$;\n- etc.\n\nSo, as a consequence, if $H_0$ is false, then (hopefully) the $p$-values are biased towards 0.\n\nSee <http://varianceexplained.org/statistics/interpreting-pvalue-histogram/>.\n\n\n\n## One-sided tests\n\nAll of the tests thus far are called *two-sided* tests.\nSometimes we wish to measure the evidence (against $H_0$) in one direction only.\n\n::: {#exm-onesidedtests}\nSuppose $X_1,\\dots,X_n\\iid\\N(\\mu,\\sigma^2)$ with $\\sigma^2$ known. \nConsider testing $H_0: \\mu \\leq 0$.\n<!-- In Example \\ref{eg:normalknownvariance}, we may only care about the reduction in blood pressure. -->\nThe unrestricted MLE remains $\\hat\\mu = \\bar X$, but the restricted MLE under $H_0$ is a bit tricky.\nWith a little bit of reasoning,\n$$\n\\tilde\\mu=\\begin{cases}\n\\bar X & \\bar X \\leq 0 \\\\\n0 & \\bar X > 0\n\\end{cases}\n$$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nn <- 10\nsigma <- 4.3\ntibble(\n  mu = seq(-6, -2, length = 100),\n  ll = dnorm(-4, mean = mu, sd = sigma, log = TRUE)\n) %>%\n  mutate() %>%\n  ggplot() +\n  annotate(\"rect\", xmin = -Inf,  xmax = 0, ymin = -Inf, ymax = Inf,\n            alpha = 0.2) +\n  geom_line(aes(mu, ll, col = \"a\")) +\n  geom_line(aes(mu + 5, ll, col = \"b\")) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n\n  scale_y_continuous(breaks = NULL, name = expression(log~L(mu))) +\n  scale_x_continuous(breaks = 0, name = expression(mu)) +\n  guides(col = \"none\") \n```\n\n::: {.cell-output-display}\n![](chapter6_files/figure-epub/fig-onesidedtestmle-1.png){#fig-onesidedtestmle}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore, the log LR statistic depends on the value of $\\bar X$:\n$$\n\\log W_{LR} = \\ell(\\hat\\mu|\\bX) - \\ell(\\tilde \\mu|\\bX) = \\begin{cases}\n0 & \\bar X \\leq 0 \\\\\n\\frac{n\\bar X^2}{2\\sigma^2} & \\bar X > 0\n\\end{cases}\n$$\n(the second case when $\\bar X>0$ is as before).\nThe $p$-value from data $\\bx$, using the monotonicity of $\\bar X$ in the LRT statistic, is\n$$\np(\\bx) = \\begin{cases}\n1 &\\bar x \\leq 0 \\\\\n\\Pr(\\bar X > \\bar x) = 1-\\Phi(\\sqrt n \\bar x / \\sigma) &\\bar x > 0 \n\\end{cases}\n$$\nHence, relative to the 'two-sided' test that we saw previously, the $p$-value is *halved* if $\\bar x > 0$, and ignores the precise value of $\\bar x$ if $\\bar x \\leq 0$.\n\n:::  \n\nFurther remarks:\n\n1. Performing a one-sided test instead of a two-sided test thus makes any apparent evidence against $H_0$ seem stronger (since the $p$-value is halved).\n\n2. In practice there are rather few situations where performing a one-sided test, which assumes that we know in advance that departures from $H_0$ are in one direction only, can be justified. When assessing the effect of a new drug, for example, the convention is to assess evidence for an effect in either direction, positive or negative.\n\n3. The two-sided test is said to be more *conservative* than the one-sided test: The one-sided test risks over-stating the strength of evidence against $H_0$ if the underlying assumption--that evidence against $H_0$ counts in one direction only--is actually false.\n\n\n## \"Failing to reject the null hypothesis\"\n\n> Absence of proof is not proof of absence. You are not able prove a negative.\n\n1. Australian Tree Lobsters were assumed to be extinct. There was no evidence that any were still living because no one had seen them for decades. Yet in 1960, scientists observed them. \n\n2. In criminal trial, we start with the assumption that the defendant is innocent until proven guilty. If the prosecutor fails to meet a an evidentiary standard, it does not mean the defendant is innocent.\n\n::: {.callout-warning}\n### Accepting the null hypothesis\nAccepting the null hypothesis indicates that you have proven that an effect does not exist. Maybe, this is what you mean?^[https://statisticsbyjim.com/hypothesis-testing/failing-reject-null-hypothesis/]\n:::\n\n## Asymptotic distribution of LRT: An experiment\n\nLet's try to \"verify\" the distribution of the test statistic $2\\log \\lambda(\\bX)$.\n\n1. Draw $X_1,\\dots,X_{10}\\sim\\N(8,1)$\n2. Compute $T(\\bX) = 2\\log \\lambda(\\bX) = \\sum_{i=1}^n (X_i-\\bar X)^2$\n3. Repeat steps 1--2 $B=10000$ number of times to get $T_1,\\dots,T_B$\n\nWe can plot the histogram of the observed test statistic, and overlay a $\\chi_9^2$ density over it.\nAs can be seen, it is a good fit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nB <- 10000\nres <- rep(NA, B)\nfor (i in 1:B) {\n  X <- rnorm(10, mean = 8)\n  res[i] <- sum((X - mean(X)) ^ 2)\n}\nggplot() +\n  geom_histogram(aes(x = res, y = ..density..), col = \"white\") +\n  geom_line(data = tibble(x = seq(0, 35, length = 100),\n                          y = dchisq(x, 10 - 1)),\n            aes(x, y), col = \"red3\", size = 1) +\n  scale_y_continuous(breaks = NULL) +\n  labs(x = expression(2~log~lambda(X)), y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](chapter6_files/figure-epub/fig-asymptoticdist-1.png){#fig-asymptoticdist}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActually, in this particular case, the distribution of $2\\log\\lambda(\\bX)$ is **exact**.\nNote that\n$$\n2 \\log \\lambda(\\bX) = \\frac{n-1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2 = (n-1)S^2\n$$\nwhich is the sample variance.\nWe've seen previously that\n$$\n\\frac{(n-1)S^2}{\\sigma^2} \\sim\\chi^2_{n-1}.\n$$\nThus, $2\\log \\lambda(\\bX)$ is merely a *scaled* $\\chi^2$ distribution (but in this case $\\sigma^2=1$).\n\n",
    "supporting": [
      "chapter6_files/figure-epub"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}