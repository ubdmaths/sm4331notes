{
  "hash": "ae866e3e1d6ac78f69bdb33ab60d9c52",
  "result": {
    "engine": "knitr",
    "markdown": "\\usepackage{blkarray}\n\n\\newcommand{\\bzero}{{\\mathbf 0}}\n\\newcommand{\\bone}{{\\mathbf 1}}\n\\newcommand{\\ba}{{\\mathbf a}}\n\\newcommand{\\bb}{{\\mathbf b}}\n\\newcommand{\\bc}{{\\mathbf c}}\n\\newcommand{\\bd}{{\\mathbf d}}\n\\newcommand{\\be}{{\\mathbf e}}\n\\newcommand{\\bff}{{\\mathbf f}}\n\\newcommand{\\bg}{{\\mathbf g}}\n\\newcommand{\\bh}{{\\mathbf h}}\n\\newcommand{\\bi}{{\\mathbf i}}\n\\newcommand{\\bj}{{\\mathbf j}}\n\\newcommand{\\bk}{{\\mathbf k}}\n\\newcommand{\\bl}{{\\mathbf l}}\n\\newcommand{\\bmm}{{\\mathbf m}}\n\\newcommand{\\bn}{{\\mathbf n}}\n\\newcommand{\\bo}{{\\mathbf o}}\n\\newcommand{\\bp}{{\\mathbf p}}\n\\newcommand{\\bq}{{\\mathbf q}}\n\\newcommand{\\br}{{\\mathbf r}}\n\\newcommand{\\bs}{{\\mathbf s}}\n\\newcommand{\\bt}{{\\mathbf t}}\n\\newcommand{\\bu}{{\\mathbf u}}\n\\newcommand{\\bv}{{\\mathbf v}}\n\\newcommand{\\bw}{{\\mathbf w}}\n\\newcommand{\\bx}{{\\mathbf x}}\n\\newcommand{\\by}{{\\mathbf y}}\n\\newcommand{\\bz}{{\\mathbf z}}\n\\newcommand{\\bA}{{\\mathbf A}}\n\\newcommand{\\bB}{{\\mathbf B}}\n\\newcommand{\\bC}{{\\mathbf C}}\n\\newcommand{\\bD}{{\\mathbf D}}\n\\newcommand{\\bE}{{\\mathbf E}}\n\\newcommand{\\bF}{{\\mathbf F}}\n\\newcommand{\\bG}{{\\mathbf G}}\n\\newcommand{\\bH}{{\\mathbf H}}\n\\newcommand{\\bI}{{\\mathbf I}}\n\\newcommand{\\bJ}{{\\mathbf J}}\n\\newcommand{\\bK}{{\\mathbf K}}\n\\newcommand{\\bL}{{\\mathbf L}}\n\\newcommand{\\bM}{{\\mathbf M}}\n\\newcommand{\\bN}{{\\mathbf N}}\n\\newcommand{\\bO}{{\\mathbf O}}\n\\newcommand{\\bP}{{\\mathbf P}}\n\\newcommand{\\bQ}{{\\mathbf Q}}\n\\newcommand{\\bR}{{\\mathbf R}}\n\\newcommand{\\bS}{{\\mathbf S}}\n\\newcommand{\\bT}{{\\mathbf T}}\n\\newcommand{\\bU}{{\\mathbf U}}\n\\newcommand{\\bV}{{\\mathbf V}}\n\\newcommand{\\bW}{{\\mathbf W}}\n\\newcommand{\\bX}{{\\mathbf X}}\n\\newcommand{\\bY}{{\\mathbf Y}}\n\\newcommand{\\bZ}{{\\mathbf Z}}\n\\newcommand{\\balpha}{{\\boldsymbol\\alpha}}\n\\newcommand{\\bbeta}{{\\boldsymbol\\beta}}\n\\newcommand{\\bgamma}{{\\boldsymbol\\gamma}}\n\\newcommand{\\bdelta}{{\\boldsymbol\\delta}}\n\\newcommand{\\bepsilon}{{\\boldsymbol\\epsilon}}\n\\newcommand{\\bvarepsilon}{{\\boldsymbol\\varepsilon}}\n\\newcommand{\\bzeta}{{\\boldsymbol\\zeta}}\n\\newcommand{\\bfeta}{{\\boldsymbol\\eta}}\n\\newcommand{\\boldeta}{{\\boldsymbol\\eta}}\n\\newcommand{\\btheta}{{\\boldsymbol\\theta}}\n\\newcommand{\\bvartheta}{{\\boldsymbol\\vartheta}}\n\\newcommand{\\biota}{{\\boldsymbol\\iota}}\n\\newcommand{\\bkappa}{{\\boldsymbol\\kappa}}\n\\newcommand{\\blambda}{{\\boldsymbol\\lambda}}\n\\newcommand{\\bmu}{{\\boldsymbol\\mu}}\n\\newcommand{\\bnu}{{\\boldsymbol\\nu}}\n\\newcommand{\\bxi}{{\\boldsymbol\\xi}}\n\\newcommand{\\bpi}{{\\boldsymbol\\pi}}\n\\newcommand{\\bvarpi}{{\\boldsymbol\\varpi}}\n\\newcommand{\\brho}{{\\boldsymbol\\rho}}\n\\newcommand{\\bvarrho}{{\\boldsymbol\\varrho}}\n\\newcommand{\\bsigma}{{\\boldsymbol\\sigma}}\n\\newcommand{\\bvarsigma}{{\\boldsymbol\\varsigma}}\n\\newcommand{\\btau}{{\\boldsymbol\\tau}}\n\\newcommand{\\bupsilon}{{\\boldsymbol\\upsilon}}\n\\newcommand{\\bphi}{{\\boldsymbol\\phi}}\n\\newcommand{\\bvarphi}{{\\boldsymbol\\varphi}}\n\\newcommand{\\bchi}{{\\boldsymbol\\chi}}\n\\newcommand{\\bpsi}{{\\boldsymbol\\psi}}\n\\newcommand{\\bomega}{{\\boldsymbol\\omega}}\n\\newcommand{\\bGamma}{{\\boldsymbol\\Gamma}}\n\\newcommand{\\bDelta}{{\\boldsymbol\\Delta}}\n\\newcommand{\\bTheta}{{\\boldsymbol\\Theta}}\n\\newcommand{\\bLambda}{{\\boldsymbol\\Lambda}}\n\\newcommand{\\bXi}{{\\boldsymbol\\Xi}}\n\\newcommand{\\bPi}{{\\boldsymbol\\Pi}}\n\\newcommand{\\bSigma}{{\\boldsymbol\\Sigma}}\n\\newcommand{\\bUpsilon}{{\\boldsymbol\\Upsilon}}\n\\newcommand{\\bPhi}{{\\boldsymbol\\Phi}}\n\\newcommand{\\bPsi}{{\\boldsymbol\\Psi}}\n\\newcommand{\\bOmega}{{\\boldsymbol\\Omega}}\n\\DeclareMathOperator{\\diag}{diag}\n\\DeclareMathOperator{\\Prob}{P}\n\\DeclareMathOperator{\\E}{E}\n\\DeclareMathOperator{\\Var}{Var}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\Corr}{Corr}\n\\DeclareMathOperator{\\sd}{sd}\n\\DeclareMathOperator{\\se}{se}\n\\DeclareMathOperator{\\N}{N}\n\\DeclareMathOperator{\\Bin}{Bin}\n\\DeclareMathOperator{\\Bern}{Bern}\n\\DeclareMathOperator{\\Dir}{Dir}\n\\DeclareMathOperator{\\Wis}{Wis}\n\\DeclareMathOperator{\\logit}{logit}\n\\DeclareMathOperator{\\expit}{expit}\n\\DeclareMathOperator{\\Mult}{Mult}\n\\DeclareMathOperator{\\Cat}{Cat}\n\\DeclareMathOperator{\\Pois}{Poi}\n\\DeclareMathOperator{\\Geom}{Geom}\n\\DeclareMathOperator{\\NBin}{NBin}\n\\DeclareMathOperator{\\Exp}{Exp}\n\\DeclareMathOperator{\\Betadist}{Beta}\n\\DeclareMathOperator{\\Hypergeom}{Hypergeom}\n\\DeclareMathOperator{\\Cauchy}{Cauchy}\n\\DeclareMathOperator{\\hCauchy}{half-Cauchy}\n\\DeclareMathOperator{\\LKJ}{LKJ}\n\\DeclareMathOperator{\\Unif}{Unif}\n\\DeclareMathOperator{\\KL}{KL}\n\\DeclareMathOperator{\\ind}{\\mathds{1}}\n\\newcommand{\\iid}{\\,\\overset{\\text{iid}}{\\sim}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\DeclareMathOperator{\\Lik}{L}\n\\DeclareMathOperator{\\argmax}{argmax}\n\\DeclareMathOperator{\\vecc}{vec}\n\\DeclareMathOperator{\\dd}{d}\n\\newcommand{\\dint}{\\dd\\hspace{0.5pt}\\!}\n\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\bbZ}{\\mathbb{Z}}\n\\newcommand{\\bbC}{\\mathbb{C}}\n\\newcommand{\\bbS}{\\mathbb{S}}\n\\newcommand{\\bbH}{\\mathbb{H}}\n\\newcommand{\\bbP}{\\mathbb{P}}\n\n\\newcommand{\\cA}{{\\mathcal A}}\n\\newcommand{\\cB}{{\\mathcal B}}\n\\newcommand{\\cC}{{\\mathcal C}}\n\\newcommand{\\cD}{{\\mathcal D}}\n\\newcommand{\\cE}{{\\mathcal E}}\n\\newcommand{\\cF}{{\\mathcal F}}\n\\newcommand{\\cG}{{\\mathcal G}}\n\\newcommand{\\cH}{{\\mathcal H}}\n\\newcommand{\\cI}{{\\mathcal I}}\n\\newcommand{\\cJ}{{\\mathcal J}}\n\\newcommand{\\cK}{{\\mathcal K}}\n\\newcommand{\\cL}{{\\mathcal L}}\n\\newcommand{\\cM}{{\\mathcal M}}\n\\newcommand{\\cN}{{\\mathcal N}}\n\\newcommand{\\cO}{{\\mathcal O}}\n\\newcommand{\\cP}{{\\mathcal P}}\n\\newcommand{\\cQ}{{\\mathcal Q}}\n\\newcommand{\\cR}{{\\mathcal R}}\n\\newcommand{\\cS}{{\\mathcal S}}\n\\newcommand{\\cT}{{\\mathcal T}}\n\\newcommand{\\cU}{{\\mathcal U}}\n\\newcommand{\\cV}{{\\mathcal V}}\n\\newcommand{\\cW}{{\\mathcal W}}\n\\newcommand{\\cX}{{\\mathcal X}}\n\\newcommand{\\cY}{{\\mathcal Y}}\n\\newcommand{\\cZ}{{\\mathcal Z}}\n\n\\newcommand{\\myoverbrace}[3][gray]{{\\color{#1}\\overbrace{\\color{black}#2}^{#3}}}\n\\newcommand{\\myunderbrace}[3][gray]{{\\color{#1}\\underbrace{\\color{black}#2}_{#3}}}\n\n\\renewcommand{\\Pr}{\\operatorname{P}}\n\n\n\n\n\n# Probability Theory Primer\n\n::: {.callout-caution}\n### Hello, Students!\nStudents, write your notes in the corresponding `.qmd` files under the `students/` folder.\nThe Editor will then merge everything into one cohesive notes.\n\nBTW, for more callout blocks, look here: <https://quarto.org/docs/authoring/callouts.html>\n:::\n\n---\ntitle: \"Introduction and Chi-square Distribution\"\nauthor: \"Aribah\"\neditor: visual\ncrossref:\n  lof-title: \"List of Figures\"\nformat: pdf\n---\n\n# Random Sampling\n\nWhen experimenting, the data collected can be represented or modeled as a set of **random variables** that describe the observed values.\n\nWe can model this by assuming $X = {X_1,\\dots, X_n}$ sampled from a population whose pdf or pmf is $f_x(x)$\n\nWe can write this as $${X_1,\\dots, X_n} \\sim f_x(x)$$\n\n::: {#nte-sampling}\nKey Assumption ðŸ“Œ: The distribution $f_x$ is imposed on data as a model.\n:::\n\n## Independent and Identically Distributed (IID) Samples\n\nThe samples are assumed to be:\n\n-   Independent: No relationship between observations.\n-   Identical Distribution: Each sample follows the same probability distribution.\n\nAs a result, the **joint probability density function (pdf)** can be written as:\n\n$$ f_X(X_1, \\dots, X_n) = \\prod_{i=1}^{n} f(X_i) $$\n\n# Definition 1 (Statistics)\n\nA statistic is any function :::\n\nLet $$\nX_1, \\dots, X_n \\sim \\text{N}(\\mu, \\sigma^2).\n$$ What is the distribution of the sum of squares?\n\nCheck out <https://quarto.org/docs/authoring/cross-references.html#theorems-and-proofs> for more div types (like theorem, lemmas, proofs, etc.).\n\n{{< lipsum 1 >}}\n\n::: {#fig-placeholder}\n{{< placeholder 400 200 >}}\n\nCaption for the image.\n:::\n\n@fig-placeholder shows an image.\n\n# Definition 5\n\nLet $Z_{1},...,Z_{k}$\n\n{{< lipsum 1 >}}\n\nWrite R code like this:\n\n```` markdown\n```{{r}}\n1 + 1\n```\n````\n\nWhich will be rendered as:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 + 1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2\n```\n\n\n:::\n:::\n\n\n\n::: {.callout-tip}\nFeel free to rearrange the sections however you wish.\nBelow, you will find some extra bits that you may want to include as part of your notes, or just leave them as the appendix.\n:::\n\n--------------\n\n## Algebras of sets\n\nThis is the mathematical structure that allows us to *observe* and *measure* random events.\nLogically,\n\n1. If an event $A$ can be observed, then its complement can be too. I.e. $A \\in \\cF \\Rightarrow A^c \\in \\cF$.\n\n2. At least one outcome can be observed, i.e. $\\Omega \\in \\cF$.\n\n3. If two or more events are observed, then at least one of them (or both) can be observed, i.e.\n$$\nA,B \\in \\cF \\Rightarrow A \\cup B \\in \\cF\n$$\n\nIf 1--3 holds, then $\\cF$ is said to be an *algebra* over $\\Omega$.\nIn addition, if you can \"add\" up infinitely many countable things, $\\cF$ is called a *$\\sigma$-algebra*.\n$$\nA_1,A_2,\\dots \\in\\cF \\Rightarrow \\bigcup_{i=1}^\\infty A_i \\in \\cF\n$$\n\n## Why $\\sigma$-algebra?\n\nIn probability theory and statistics, an experiment (or trial) is a procedure that can be repeated and has a well-defined set of possible outcomes (known as the sample space $\\Omega$).\nEvents are thought of as being subsets of $\\Omega$, while probabilities are merely a mapping from some *event space* $\\mathcal F$ to $[0,1]$.\n\nTo make this idea concrete, for the die roll example, $\\Omega=\\{1,\\dots,6\\}$, while an event could be $E=\\{2,4,6\\}\\subset \\Omega$ (getting an even number).\nThe probability of the event $E$ occurring is $\\Pr(E)=\\frac{1}{2}$--so it indeed behaves like a function, taking input some event and spitting out a number between 0 and 1.\n\nNote here that $\\mathcal F$ is not $\\Omega$--it has to be bigger than $\\Omega$ as we're not just interested in singleton outcomes.\nA good starting point would be $\\mathcal F = \\mathcal P(\\Omega)$, the set of all subsets of $\\Omega$, which should contain all possible events constructed from the set of outcomes.\n\n### Rules of probability\n\nHaving abstracted the notion of $\\Omega$ and $\\mathcal F$, we should also define some rules that the probability function $\\Pr:\\mathcal P(\\Omega)\\to[0,1]$ must follow.\nLet us list down a few:\n\ni. $\\Pr(E) \\geq 0, \\forall E$;\nii. $\\Pr(\\varnothing)=0$ and $\\Pr(\\Omega) = 1$;^[$\\varnothing = \\{ \\}$ is the empty set.]\niii. If $E_1 \\cap E_2 = \\varnothing$, then $\\Pr(E_1 \\cup E_2)=\\Pr(E_1) + \\Pr(E_2)$; and\niv. If $E_1, E_2,\\dots$ are mutually disjoint events, then $\\Pr\\Big(\\bigcup_{i=1}^\\infty E_i \\Big) = \\sum_{i=1}^\\infty \\Pr(E_i)$.\n\nIndeed, these are quite logical impositions that ensure we don't end up with nonsensical probabilities.\nFor instance, by ii. and iii., modelling a (biased) coin toss by $\\Pr(H)=0.7$ necessitates $\\Pr(T)=0.3$ and not anything else, e.g. $\\Pr(T)=0.5$. \n\n### The need for measure theory\n\nWe've managed to come up with probability rules so far without the need for measure theory, so what's the problem?\nThe problem is that in the way that we've described it, this is actually too much to ask!\nThere will be instances where this whole framework fails and we can't assign probabilities properly, especially when we need it the most.\n\nConsider that, with all these demands, we can't even define the uniform random variable on $\\Omega=[0,1]$!\nThat is, no mapping $\\Pr:\\mathcal P([0,1])\\to[0,1]$ exists such that $\\Pr([a,b])=b-a$ for $0\\leq a \\leq b \\leq 1$ which satisfies all of the rules i. to iv. listed above.\nFor a proof, see the appendix.\nEvidently some concession has to be made (which one?), and the probability map must be constructed more carefully.\nThe answer lies in measure theory.\n\n## An unmeasurable set\n\nAs mentioned, we are unable to define a uniform probability measure on the unit interval, given by\n$$\n\\Pr([a,b]) = b-a\n$$\nthat satisfies all the probability rules listed in i. to iv. earlier.\nOn the face of it, all the rules themselves are satisfied: $\\Pr(\\Omega)=\\Pr([0,1])=1$, $\\Pr(\\varnothing)=\\Pr([a,a])=0$ (for any $a\\in[0,1]$), and certainly probabilities of disjoint subsets of $[0,1]$ are just the sum of the lengths of the intervals.\n\nThese are all great properties to have, so we must concede instead on the domain of the probability function, i.e. the event space.\nThe proof of the proposition below in instructive, in that it illustrates the existence of a \"non-measurable\" set.\nThat is, there are such events (subsets in $[0,1]$) for which we are unable to assign probabilities to.\n\n::: {#prp-measure-theory}\nThere does not exist a definition of $\\Pr:\\mathcal P([0,1])\\to[0,1]$ satisfying $\\Pr([a,b])=b-a$ and i. to iv. (as listed earlier).\n:::\n\n::: {.proof}\nAll we need to show is the existence of one such subset of $[0,1]$ whose measure is undefined. The set we are about to construct is called the Vitali set^[https://en.wikipedia.org/wiki/Vitali_set], after Giuseppe Vitali who described it in 1905.\n\nBefore proceeding, we introduce some notation.\nFor a uniform measure on $[0,1]$, one expects that the measure of some subset $A \\subseteq [0,1]$ to be unaffected by \"shifting\" (with wrap-around) of that subset by some fixed amount $r\\in[0,1]$.\nDefine the *$r$-shift* of $A\\subseteq [0,1]$ by\n$$\nA \\oplus r := \\left\\{ a + r \\mid a \\in A, a+r \\leq 1 \\right\\} \\cup \\left\\{ a + r - 1 \\mid a \\in A, a+r > 1 \\right\\}.\n$$\nThen we should have\n$$\n\\Pr(A \\oplus r) = \\Pr(A). \n$$\nFor example, $\\Pr([0.7, 0.9] \\oplus 0.2) = \\Pr([0.9,1] \\cup [0,0.1]) = 0.2$.\n\nNow, define an equivalence relation on $[0,1]$ by the following: \n$$x\\sim y \\Rightarrow y-x \\in \\mathbb Q$$\nThat is, two real numbers $x$ and $y$ are deemed to be similar if their difference is a rational number. \nThe intent is to segregate all the real numbers $x\\in[0,1]$ by this equivalence relation, and collect them into groups called equivalence classes, denoted by $[x]$. \nHere, $[x]$ is the set $\\{y \\in [0,1] \\mid x \\sim y\\}.$ \nFor instance, \n\n- The equivalence class of $0$ is the set of real numbers $x$ such that $x \\sim 0$, i.e. $[0] = \\{y \\in [0,1] \\mid y-0\\in\\mathbb Q \\}$, which is the set of all rational numbers in $[0,1]$.\n\n- The equivalence class of an irrational number $z_1\\in[0,1]$ is clearly not in $[0]$, thus would represent a different equivalence class $[z_1]=\\{y \\in [0,1] \\mid y-z_1 \\in \\mathbb Q \\}$.\n\n- Yet another irrational number $z_2\\not\\in [z_1]$ would exist, i.e. a number $z_2\\in[0,1]$ such that $z_2-z_1 \\not\\in\\mathbb Q$, and thus would represent a different equivalence class $[z_2]$.\n\n- And so on... \n\nThe equivalence classes may therefore be represented by $[0],[z_1],[z_2],\\dots$ where $z_i$ are all irrational numbers that differ by an irrational number, and there are uncountably many such numbers, and therefore classes. \n\nConstruct the Vitali set $V$ as follows: Take precisely one element from each equivalent class, and put it in $V$. \nAs a remark, such a $V$ must surely exist by the Axiom of Choice^[Given a collection of non-empty sets, it is always possible to construct a new set by taking one element from each set in the original collection. See https://brilliant.org/wiki/axiom-of-choice/]. \n\n\nConsider now the union of shifted Vitali sets by some rational value $r\\in[0,1]$,\n$$\n\\bigcup_{r} (V \\oplus r)\n$$\nAs a reminder, the set of rational numbers is countably infinite^[https://www.homeschoolmath.net/teaching/rational-numbers-countable.php]. \nWe make two observations:\n\n1. **The equivalence relation partitions the interval $[0,1]$ into a disjoint union of equivalence classes.** In other words, the sets $(V \\oplus r)$ and $(V \\oplus s)$ are disjoint for any rationals $r\\neq s$, such that $r,s\\in[0,1]$. If they were not disjoint, this would mean that there exists some $x,y\\in[0,1]$ with $x+r\\in(V \\oplus r)$ and $y+s\\in (V \\oplus s)$ such that $x+r=y+s$. But then this means that $x-y=s-r\\in\\mathbb Q$ so $x$ and $y$ are in the same equivalent class, and this is a contradiction. Importantly,\n\\begin{equation}\\label{eq:contr1}\n\\Pr\\left(\\bigcup_{r} (V \\oplus r)\\right) = \\sum_r \\Pr(V \\oplus r) = \\sum_r \\Pr(V)\n\\end{equation}\n\n2. **Every point in $[0,1]$ is contained in the union $\\bigcup_{r} (V \\oplus r)$.** To see this, fix a point $x$ in $[0,1]$. Note that this point belongs to some equivalent class of $x$, and in this equivalence class there exists some point $\\alpha$ which belongs to $V$ as well by construction. Hence, $\\alpha \\sim x$, and thus $x-\\alpha=r\\in\\mathbb Q$, implying that $x$ is a point in the Vitali set $V$ shifted by $r$. Therefore, $$[0,1] \\subseteq  \\bigcup_{r} (V \\oplus r).$$ and we may write $$1 = \\Pr([0,1]) \\leq \\Pr\\left(\\bigcup_{r} (V \\oplus r)\\right)\\leq 1,$$ since the measure of any set contained in another must have smaller or equal measure (a relation implied by property iii.^[Let $A$ and $B$ be such that $A \\subseteq B$. Then we may write $B = A \\cup (B\\setminus A)$ where the sets $A$ and $B \\setminus A$ are disjoint. Hence, $\\Pr(B)=\\Pr(A)+\\Pr(B \\setminus A)$, and since probabilities are non-negative, we have that $\\Pr(B)\\geq \\Pr(A)$.]) \nas well as all probabilities are less than equal to 1^[For any $A$, $\\Pr(\\Omega)=\\Pr(A \\cup A^c) = \\Pr(A) + \\Pr(A^c) = 1$, so $\\Pr(A)\\leq 1$.]. We see that\n\\begin{equation}\\label{eq:contr2}\n\\Pr\\left(\\bigcup_{r} (V \\oplus r)\\right) = 1.\n\\end{equation}\n\nEquating \\eqref{eq:contr1} and \\eqref{eq:contr2} together, we find a contradiction:\nA countably infinite sum of a constant value can only equal $0$, $+\\infty$ or $-\\infty$, but never 1.\n\n:::\n\n## Conditional probability\n\n> The latest estimate puts the proportion of geology students at FOS to be 5%. A randomly selected student from FOS, Nafeesah, is described by her peers as someone who loves the outdoors and gets overly excited when shown something that is related to rocks.\n\nWhich statement is more likely?\n\nA.  Nafeesah is undertaking a BSc Geology programme.\nB.  Nafeesah is not undertaking a BSc Geology programme.\n\nLet\n\n- $E$ be the 'evidence'\n- $G$ be the event that a student takes Geology\n\nThen\n$$\n\\Pr(G|E) = \\frac{\\Pr(E|G)\\Pr(G)}{\\Pr(E)} \\approx \\frac{0.05}{\\Pr(E)}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(1e-10, 1, length = 100)\nplot_df <- tibble(\n  x = x,\n  y = 0.05 / x\n)\n\nggplot(plot_df, aes(x, y)) +\n  geom_line() +\n  geom_segment(x = -Inf, xend = 0.05 / 0.5, y = 0.5,yend = 0.5,\n               linetype = \"dashed\", col = \"red3\") +\n  geom_segment(x = 0.05 / 0.5, xend = 0.05 / 0.5, y = 0.5, yend = -Inf,\n               linetype = \"dashed\", col = \"red3\") +\n  scale_y_continuous(limits = c(0, 1)) +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.2)) +\n  labs(x = \"P(E)\", y = \"P(G|E)\")\n```\n\n::: {.cell-output-display}\n![](chapter1_files/figure-epub/base_rate_fallacy-1.png)\n:::\n:::\n\n\n\n## Bayesian statistics\n\nSometime between 1746 and 1749, Rev. Thomas Bayes conducted this experiment.\n\n> Imagine a square, flat table. You throw a marker (e.g. a coin) but do not know where it lands. You ask an assistant to randomly throw a ball on the table. The assistant informs you whether it stopped to the left or right from the first ball. How to use this information to better estimate where your marker landed?\n\nThe Bayesian principle is about updating beliefs.\n\n- Let $X \\in [0,1]$ be the location of the ball on a horizontal axis.\n- Before any new information, any position $X$ is possible, say $X\\sim\\Unif(0,1)$.\n- Let $Y$ be the number of times the assistant's ball landed left of the marker after $n$ throws. Then $Y|X \\sim \\Bin(n,X)$.\n- What we want is information regarding $X|Y$, which is obtained using Bayes Theorem\n$$\n\\Pr(X\\in x|Y=y) = \\frac{\\Pr(Y=y|X\\in x)\\Pr(X\\in x)}{\\Pr(Y=y)}\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# https://dosreislab.github.io/2019/01/27/ballntable.html\nset.seed(123)\nn <- 15\nxy <- runif(2) # position of coin after intial throw\nxy.2 <- matrix(runif(2 * n), ncol=2) # additional n throws of the ball\n\npos <- numeric(2)\npos[1] <- sum(xy.2[,1] < xy[1])\npos[2] <- sum(xy.2[,2] < xy[2])\n\njointf <- function(pos = pos, n = n, N=100) {\n  a <- pos[1]; b <- pos[2]\n  x <- y <- seq(from=0, to=1, len=N)\n  xf <- x^a * (1-x)^(n-a)\n  yf <- y^b * (1-y)^(n-b)\n  z <- xf %o% yf\n}\n\nCf <- function(x, y, n) {\n  ( factorial(n+1) )^2 /\n  ( factorial(x) * factorial(n-x) * factorial(y) * factorial(n-y) )\n}\n\nz <- jointf(pos, n) * Cf(pos[1], pos[2], n)\n\nas.data.frame(z) %>%\n  `colnames<-`(1:100 / 100) %>%\n  rownames_to_column() %>%\n  gather(key, value, -rowname) %>%\n  mutate(rowname = as.numeric(rowname) / 100,\n         key = as.numeric(key)) %>%\n  ggplot(aes(rowname, key, z = value)) +\n  geom_contour() +\n  geom_point(x = xy[1], y = xy[2]) +\n  lims(x = c(0, 1), y = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](chapter1_files/figure-epub/bayes_ball-1.png)\n:::\n:::\n\n\n\n## Probability integral transform\n\n::: {#thm-pit}\nLet $X$ have continuous cdf $F_X(x)$ and define the random variable $Y$ as $Y=F_X(X)$. Then $Y$ is uniformly distributed on $(0,1)$, that is $f_Y(y)=1 \\ \\forall y\\in[0,1]$ \nwith $\\Pr(Y\\leq y)=y$.\n:::\n\n::: {.proof}\n\\begin{align*}\n\\Pr(Y \\leq y) \n&= \\Pr(F_X(X) \\leq y) \\\\\n&= \\Pr\\big(X \\leq F_X^{-1}(y)\\big) \\\\\n&= F_X\\big(F_X^{-1}(y) \\big) = y. \n\\end{align*}\n:::\n\nThe PIT is a special kind of transformation, useful for various statistical purposes.\nSuppose we wish to generate $X\\sim F_X$--this is done via $X=F_X^{-1}(U)$ where $U\\sim\\Unif(0,1)$.\n",
    "supporting": [
      "chapter1_files/figure-epub"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}