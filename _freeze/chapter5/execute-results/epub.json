{
  "hash": "7f31fd412b6282f5ac475be879b448f8",
  "result": {
    "engine": "knitr",
    "markdown": "\\usepackage{blkarray}\n\n\\newcommand{\\bzero}{{\\mathbf 0}}\n\\newcommand{\\bone}{{\\mathbf 1}}\n\\newcommand{\\ba}{{\\mathbf a}}\n\\newcommand{\\bb}{{\\mathbf b}}\n\\newcommand{\\bc}{{\\mathbf c}}\n\\newcommand{\\bd}{{\\mathbf d}}\n\\newcommand{\\be}{{\\mathbf e}}\n\\newcommand{\\bff}{{\\mathbf f}}\n\\newcommand{\\bg}{{\\mathbf g}}\n\\newcommand{\\bh}{{\\mathbf h}}\n\\newcommand{\\bi}{{\\mathbf i}}\n\\newcommand{\\bj}{{\\mathbf j}}\n\\newcommand{\\bk}{{\\mathbf k}}\n\\newcommand{\\bl}{{\\mathbf l}}\n\\newcommand{\\bmm}{{\\mathbf m}}\n\\newcommand{\\bn}{{\\mathbf n}}\n\\newcommand{\\bo}{{\\mathbf o}}\n\\newcommand{\\bp}{{\\mathbf p}}\n\\newcommand{\\bq}{{\\mathbf q}}\n\\newcommand{\\br}{{\\mathbf r}}\n\\newcommand{\\bs}{{\\mathbf s}}\n\\newcommand{\\bt}{{\\mathbf t}}\n\\newcommand{\\bu}{{\\mathbf u}}\n\\newcommand{\\bv}{{\\mathbf v}}\n\\newcommand{\\bw}{{\\mathbf w}}\n\\newcommand{\\bx}{{\\mathbf x}}\n\\newcommand{\\by}{{\\mathbf y}}\n\\newcommand{\\bz}{{\\mathbf z}}\n\\newcommand{\\bA}{{\\mathbf A}}\n\\newcommand{\\bB}{{\\mathbf B}}\n\\newcommand{\\bC}{{\\mathbf C}}\n\\newcommand{\\bD}{{\\mathbf D}}\n\\newcommand{\\bE}{{\\mathbf E}}\n\\newcommand{\\bF}{{\\mathbf F}}\n\\newcommand{\\bG}{{\\mathbf G}}\n\\newcommand{\\bH}{{\\mathbf H}}\n\\newcommand{\\bI}{{\\mathbf I}}\n\\newcommand{\\bJ}{{\\mathbf J}}\n\\newcommand{\\bK}{{\\mathbf K}}\n\\newcommand{\\bL}{{\\mathbf L}}\n\\newcommand{\\bM}{{\\mathbf M}}\n\\newcommand{\\bN}{{\\mathbf N}}\n\\newcommand{\\bO}{{\\mathbf O}}\n\\newcommand{\\bP}{{\\mathbf P}}\n\\newcommand{\\bQ}{{\\mathbf Q}}\n\\newcommand{\\bR}{{\\mathbf R}}\n\\newcommand{\\bS}{{\\mathbf S}}\n\\newcommand{\\bT}{{\\mathbf T}}\n\\newcommand{\\bU}{{\\mathbf U}}\n\\newcommand{\\bV}{{\\mathbf V}}\n\\newcommand{\\bW}{{\\mathbf W}}\n\\newcommand{\\bX}{{\\mathbf X}}\n\\newcommand{\\bY}{{\\mathbf Y}}\n\\newcommand{\\bZ}{{\\mathbf Z}}\n\\newcommand{\\balpha}{{\\boldsymbol\\alpha}}\n\\newcommand{\\bbeta}{{\\boldsymbol\\beta}}\n\\newcommand{\\bgamma}{{\\boldsymbol\\gamma}}\n\\newcommand{\\bdelta}{{\\boldsymbol\\delta}}\n\\newcommand{\\bepsilon}{{\\boldsymbol\\epsilon}}\n\\newcommand{\\bvarepsilon}{{\\boldsymbol\\varepsilon}}\n\\newcommand{\\bzeta}{{\\boldsymbol\\zeta}}\n\\newcommand{\\bfeta}{{\\boldsymbol\\eta}}\n\\newcommand{\\boldeta}{{\\boldsymbol\\eta}}\n\\newcommand{\\btheta}{{\\boldsymbol\\theta}}\n\\newcommand{\\bvartheta}{{\\boldsymbol\\vartheta}}\n\\newcommand{\\biota}{{\\boldsymbol\\iota}}\n\\newcommand{\\bkappa}{{\\boldsymbol\\kappa}}\n\\newcommand{\\blambda}{{\\boldsymbol\\lambda}}\n\\newcommand{\\bmu}{{\\boldsymbol\\mu}}\n\\newcommand{\\bnu}{{\\boldsymbol\\nu}}\n\\newcommand{\\bxi}{{\\boldsymbol\\xi}}\n\\newcommand{\\bpi}{{\\boldsymbol\\pi}}\n\\newcommand{\\bvarpi}{{\\boldsymbol\\varpi}}\n\\newcommand{\\brho}{{\\boldsymbol\\rho}}\n\\newcommand{\\bvarrho}{{\\boldsymbol\\varrho}}\n\\newcommand{\\bsigma}{{\\boldsymbol\\sigma}}\n\\newcommand{\\bvarsigma}{{\\boldsymbol\\varsigma}}\n\\newcommand{\\btau}{{\\boldsymbol\\tau}}\n\\newcommand{\\bupsilon}{{\\boldsymbol\\upsilon}}\n\\newcommand{\\bphi}{{\\boldsymbol\\phi}}\n\\newcommand{\\bvarphi}{{\\boldsymbol\\varphi}}\n\\newcommand{\\bchi}{{\\boldsymbol\\chi}}\n\\newcommand{\\bpsi}{{\\boldsymbol\\psi}}\n\\newcommand{\\bomega}{{\\boldsymbol\\omega}}\n\\newcommand{\\bGamma}{{\\boldsymbol\\Gamma}}\n\\newcommand{\\bDelta}{{\\boldsymbol\\Delta}}\n\\newcommand{\\bTheta}{{\\boldsymbol\\Theta}}\n\\newcommand{\\bLambda}{{\\boldsymbol\\Lambda}}\n\\newcommand{\\bXi}{{\\boldsymbol\\Xi}}\n\\newcommand{\\bPi}{{\\boldsymbol\\Pi}}\n\\newcommand{\\bSigma}{{\\boldsymbol\\Sigma}}\n\\newcommand{\\bUpsilon}{{\\boldsymbol\\Upsilon}}\n\\newcommand{\\bPhi}{{\\boldsymbol\\Phi}}\n\\newcommand{\\bPsi}{{\\boldsymbol\\Psi}}\n\\newcommand{\\bOmega}{{\\boldsymbol\\Omega}}\n\\DeclareMathOperator{\\diag}{diag}\n\\DeclareMathOperator{\\Prob}{P}\n\\DeclareMathOperator{\\E}{E}\n\\DeclareMathOperator{\\Var}{Var}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\Corr}{Corr}\n\\DeclareMathOperator{\\sd}{sd}\n\\DeclareMathOperator{\\se}{se}\n\\DeclareMathOperator{\\N}{N}\n\\DeclareMathOperator{\\Bin}{Bin}\n\\DeclareMathOperator{\\Bern}{Bern}\n\\DeclareMathOperator{\\Dir}{Dir}\n\\DeclareMathOperator{\\Wis}{Wis}\n\\DeclareMathOperator{\\logit}{logit}\n\\DeclareMathOperator{\\expit}{expit}\n\\DeclareMathOperator{\\Mult}{Mult}\n\\DeclareMathOperator{\\Cat}{Cat}\n\\DeclareMathOperator{\\Pois}{Poi}\n\\DeclareMathOperator{\\Geom}{Geom}\n\\DeclareMathOperator{\\NBin}{NBin}\n\\DeclareMathOperator{\\Exp}{Exp}\n\\DeclareMathOperator{\\Betadist}{Beta}\n\\DeclareMathOperator{\\Hypergeom}{Hypergeom}\n\\DeclareMathOperator{\\Cauchy}{Cauchy}\n\\DeclareMathOperator{\\hCauchy}{half-Cauchy}\n\\DeclareMathOperator{\\LKJ}{LKJ}\n\\DeclareMathOperator{\\Unif}{Unif}\n\\DeclareMathOperator{\\KL}{KL}\n\\DeclareMathOperator{\\ind}{\\mathds{1}}\n\\newcommand{\\iid}{\\,\\overset{\\text{iid}}{\\sim}\\,}\n\\DeclareMathOperator*{\\plim}{plim}\n\\DeclareMathOperator{\\Lik}{L}\n\\DeclareMathOperator{\\argmax}{argmax}\n\\DeclareMathOperator{\\vecc}{vec}\n\\DeclareMathOperator{\\dd}{d}\n\\newcommand{\\dint}{\\dd\\hspace{0.5pt}\\!}\n\n\\newcommand{\\bbR}{\\mathbb{R}}\n\\newcommand{\\bbN}{\\mathbb{N}}\n\\newcommand{\\bbZ}{\\mathbb{Z}}\n\\newcommand{\\bbC}{\\mathbb{C}}\n\\newcommand{\\bbS}{\\mathbb{S}}\n\\newcommand{\\bbH}{\\mathbb{H}}\n\\newcommand{\\bbP}{\\mathbb{P}}\n\n\\newcommand{\\cA}{{\\mathcal A}}\n\\newcommand{\\cB}{{\\mathcal B}}\n\\newcommand{\\cC}{{\\mathcal C}}\n\\newcommand{\\cD}{{\\mathcal D}}\n\\newcommand{\\cE}{{\\mathcal E}}\n\\newcommand{\\cF}{{\\mathcal F}}\n\\newcommand{\\cG}{{\\mathcal G}}\n\\newcommand{\\cH}{{\\mathcal H}}\n\\newcommand{\\cI}{{\\mathcal I}}\n\\newcommand{\\cJ}{{\\mathcal J}}\n\\newcommand{\\cK}{{\\mathcal K}}\n\\newcommand{\\cL}{{\\mathcal L}}\n\\newcommand{\\cM}{{\\mathcal M}}\n\\newcommand{\\cN}{{\\mathcal N}}\n\\newcommand{\\cO}{{\\mathcal O}}\n\\newcommand{\\cP}{{\\mathcal P}}\n\\newcommand{\\cQ}{{\\mathcal Q}}\n\\newcommand{\\cR}{{\\mathcal R}}\n\\newcommand{\\cS}{{\\mathcal S}}\n\\newcommand{\\cT}{{\\mathcal T}}\n\\newcommand{\\cU}{{\\mathcal U}}\n\\newcommand{\\cV}{{\\mathcal V}}\n\\newcommand{\\cW}{{\\mathcal W}}\n\\newcommand{\\cX}{{\\mathcal X}}\n\\newcommand{\\cY}{{\\mathcal Y}}\n\\newcommand{\\cZ}{{\\mathcal Z}}\n\n\\newcommand{\\myoverbrace}[3][gray]{{\\color{#1}\\overbrace{\\color{black}#2}^{#3}}}\n\\newcommand{\\myunderbrace}[3][gray]{{\\color{#1}\\underbrace{\\color{black}#2}_{#3}}}\n\n\\renewcommand{\\Pr}{\\operatorname{P}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Likelihood theory\n\n## Finding the MLE numerically\n\nHere's how we simulation $n=100$ random sample from a normal distribution with mean $\\mu=8$ and $\\sigma=1$. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- rnorm(n = 100, mean = 8)\nmean(X) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.873118\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe mean is found to be 7.87.\nHere's a plot of the log-likelihood function ($\\mu$ against $\\ell(\\mu)$):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  x = mean(X) + seq(-1, 1, length = 100)\n) |>\n  rowwise() |>\n  mutate(y = sum(dnorm(X, mean = x, log = TRUE))) |>\n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_segment(linetype = \"dashed\", x = mean(X), xend = mean(X), y = -Inf,\n               yend = sum(dnorm(unlist(X), mean = mean(X), log = TRUE)),\n               size = 0.4, col = \"gray\") +\n  labs(x = expression(mu), y = expression(l(mu)))\n```\n\n::: {.cell-output-display}\n![Log-likelihood function of the normal mean.](chapter5_files/figure-epub/fig-mlenumericalplot-1.png){#fig-mlenumericalplot}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere's how to optimise the (log-)likelihood function.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nneg_loglik <- function(theta, data = X) {\n  -1 * sum(dnorm(x = data, mean = theta, log = TRUE))\n}\n\nres <- nlminb(\n  start = 1,  # starting value\n  objective = neg_loglik, \n  control = list(\n    trace = 1  # trace the progress of the optimiser\n  ))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  0:     2505.0388:  1.00000\n  1:     555.79160:  5.00000\n  2:     143.05127:  7.87314\n  3:     143.05127:  7.87312\n  4:     143.05127:  7.87312\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nglimpse(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 6\n $ par        : num 7.87\n $ objective  : num 143\n $ convergence: int 0\n $ iterations : int 4\n $ evaluations: Named int [1:2] 6 7\n  ..- attr(*, \"names\")= chr [1:2] \"function\" \"gradient\"\n $ message    : chr \"relative convergence (4)\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Variance reduction: *Rao-Blackwell*isation\n\nIt is possible to reduce the variance of an unbiased estimator by conditioning on a sufficient statistic.\n\n::: {#thm-raoblackwell}\n### Rao-Blackwell\n\nSuppose that $\\hat\\theta(\\bX)$ is unbiased for $\\theta$, and $S(\\bX)$ is sufficient for $\\theta$.\nThen the function of $S$ defined by\n$$\n\\phi(S) = \\E_\\theta(\\hat\\theta|S) \n$$\n\ni. is a statistic, i.e. $\\phi(S)$ does not involve $\\theta$;\nii. is an unbiased statistic, i.e. $\\E(\\phi(S)) = \\theta$; and\niii. has $\\Var_\\theta(\\phi(S)) \\leq \\Var_\\theta (\\hat\\theta)$, with equality iff $\\hat\\theta$ itself is a function of $S$.\n:::  \n\nIn other words, $\\phi(S)$ is a uniformly \\underline{better} unbiased estimator for $\\theta$.\nThus the Rao-Blackwell theorem provides a systematic method of variance reduction for an estimator that is not a function of the sufficient statistic.\n\n::: {.proof}\n\ni. Since $S$ is sufficient, the distribution of $\\bX$ given $S$ does not involve $\\theta$, and hence $\\E_\\theta(\\hat\\theta(\\bX)|S)$ does not involve $\\theta$.\n\nii. $\\E(\\phi(S)) = \\E\\left[ \\E(\\hat\\theta|S) \\right] = \\E(\\hat\\theta) = \\theta$.\n\niii. Using the law of total variance, \\begin{align*}\n\\Var(\\hat\\theta) \n&= \\E\\left[ \\Var(\\hat\\theta|S) \\right] + \\Var\\left[ \\E(\\hat\\theta|S) \\right] \\\\\n&= \\E\\left[ \\Var(\\hat\\theta|S) \\right] + \\Var(\\phi(S)) \\\\\n&\\geq \\Var(\\phi(S)),\n\\end{align*} with equality iff $\\Var(\\hat\\theta|S) =0$, i.e. iff $\\hat\\theta$ is a function of $S$.\n\n:::\n\n::: {#exm-raoblackwell}\nSuppose we have data $X_1,\\dots,X_n\\iid\\Pois(\\lambda)$ pertaining to the number of road accidents per day, and we want to estimate the probability of having no accidents $\\theta = e^{-\\lambda}=\\Pr(X_i=0)$.\n\n\nAn unbiased estimator of $\\theta$ is \n$$\n\\hat\\theta(\\bX) = \\begin{cases}\n1 & X_1 =0 \\\\\n0 & \\text{otherwise,}\n\\end{cases} \n$$\nas $\\E \\hat\\theta(\\bX) = 1\\cdot\\Pr(X_1=0)=e^{-\\lambda}=\\theta$.\nBut this is likely to be a poor estimator, since it ignores the rest of the sample $X_2,X_3,\\dots,X_n$.\n\nWe can see that $S(\\bX)=\\sum_{i=1}^n X_i$ is sufficient since the joint pdf can be expressed as\n$$\nf(\\bx|\\lambda) = \\frac{1}{x_1!\\cdots x_n!} \\cdot e^{-n\\lambda}\\lambda^{\\sum_{i=1}^nx_i}.\n$$\n\nNow apply the Rao-Blackwell theorem:\n\\begin{align*}\n\\phi(S) = \\E(\\hat\\theta|S) = \\E\\Big(\\hat\\theta \\, \\Big| \\, \\sum_{i=1}^n X_i = S  \\Big) \n&= \\Pr\\Big(X_1=0  \\, \\Big| \\, \\sum_{i=1}^n X_i = S  \\Big)\\\\\n&= \\left(1 - \\frac{1}{n} \\right)^S,\n\\end{align*}\nwhere the conditional probability in the last step comes from the Poisson-binomial relationship \n(Refer Ex. Sheet 2: Suppose $X_i\\iid\\Pois(\\lambda_i)$, then $X_1\\big|(\\sum_{i=1}^nX_i=m)\\sim\\Bin(m,\\pi)$, where $\\pi=\\lambda_1/\\sum_{i=1}^n \\lambda_i$).\n\nBy the Rao-Blackwell theorem, $\\Var(\\phi)<\\Var(\\hat\\theta(\\bX))$ (strict inequality since $\\hat\\theta(\\bX)$ is not a function of $S$), so prefer $\\phi(S)$ over $\\hat\\theta(\\bX)$ as an estimator.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <- 2\nn <- 25\n(theta <- dpois(x = 0, lambda = lambda))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1353353\n```\n\n\n:::\n\n```{.r .cell-code}\nB <- 1000\nX <- matrix(rpois(n * B, lambda = lambda), ncol = B)\ntheta_hat <- apply(X, 2, function(x) as.numeric(x[1] == 0))\nphi_hat <- apply(X, 2, function(x) (1-1/n)^(sum(x)))\n\ntibble(theta_hat, phi_hat) %>%\n  pivot_longer(everything(), names_to = \"Estimator\", values_to = \"theta_hat\") %>%\n  ggplot() +\n  geom_density(aes(theta_hat, col = Estimator, fill = Estimator), alpha = 0.6) +\n  # # geom_vline(xintercept = theta, linetype = \"dashed\") +\n  # geom_vline(data = tibble(\n  #   x = c(mean(MLE), mean(MOM)),\n  #   Estimator = c(\"MLE\", \"MOM\")\n  # ), aes(xintercept = x), linetype = \"dashed\") +\n  facet_grid(. ~ Estimator) +\n  # labs(x = expression(hat(theta)), y = expression(f~(hat(theta)))) +\n  # scale_x_continuous(breaks = seq(2, 14, by = 2)) +\n  theme(legend.position = \"none\")\n  # geom_text(data = tibble(x = c(4.9, 5.85), y = c(0.45, 0.45),\n  #                         Estimator = c(\"MLE\", \"MOM\"),\n  #                         label = c(\"E(hat(theta)[ML])\", \"E(hat(theta)[MOM])\")),\n  #           aes(x, y, label = label), parse = TRUE)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-epub/fig-raoblackwellisation-1.png){#fig-raoblackwellisation}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut is $\\phi(S)=(1-1/n)^S$ unbiased? This is guaranteed by the RB theorem. Check: Since $S=\\sum_{i=1}^n X_i \\sim\\Pois(n\\lambda)$, we get\n\\begin{align*}\n\\E(\\phi(S)) &= \\sum_{s=0}^\\infty \\left(1 - \\frac{1}{n} \\right)^s  \\frac{e^{-n\\lambda}(n\\lambda)^s}{s!}\\times e^{-\\lambda}e^{\\lambda} \\\\\n&= e^{-\\lambda} \\sum_{s=0}^\\infty \\myunderbrace{\\frac{e^{-\\lambda(n-1)}[\\lambda(n-1)]^s}{s!}}{\\text{pmf of }\\Pois(\\lambda(n-1))} = e^{-\\lambda}.\n\\end{align*}\nA similar calculation can give us the variance of this estimator.\n\n:::\n\n\n## Continuity\n\nA continuous function $\\psi(x)$ is a function such that a continuous variation of $x$ induces a continuous variation of $\\psi(x)$--i.e. no jumps allowed.\n[$\\psi$ is continuous at $c$ if $\\forall \\epsilon > 0$, $\\exists \\delta > 0$ s.t. $|x-c| < \\delta \\Rightarrow |\\psi(x)-\\psi(c)| < \\epsilon$.]{.onslideh}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  x = seq(1, 3, length = 1000),\n  y = 16 - x^2\n) -> plot_df\n\nmycols <- grDevices::palette.colors(3, palette = \"Set1\")\n\nggplot() +\n  annotate(\"rect\", xmin = 2 - 0.25, xmax = 2 + 0.25, ymin = -Inf, ymax = Inf, \n           fill = mycols[1], alpha = 0.3) +\n  annotate(\"rect\", xmin = -Inf, xmax = Inf, ymin = 12 - 2, ymax = 12 + 2, \n           fill = mycols[2], alpha = 0.3)  +\n  geom_line(data = plot_df, aes(x, y)) +\n  geom_line(data = plot_df %>% filter(x >= 2 - 0.25, x <= 2 + 0.25), aes(x, y),\n            col = mycols[3], linewidth = 2) +  \n  # geom_segment(aes(x = 2, xend = 2, y = 12, yend = -Inf), linetype = \"dashed\",\n  #              size = 0.4) \n  geom_hline(yintercept = 12, linetype = \"dashed\") +\n  geom_vline(xintercept = 2, linetype = \"dashed\") +\n  scale_x_continuous(breaks = 2 + c(-0.25, 0, 0.25), \n                     labels = c(expression(\"c-\"*delta),\n                                \"c\",\n                                expression(\"c+\"*delta))) +\n  scale_y_continuous(breaks = 12 + c(-2, 0, 2), \n                     labels = c(expression(\"f(c)-\"*epsilon),\n                                \"f(c)\",\n                                expression(\"f(c)+\"*epsilon)))  \n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-epub/fig-continuity-1.png){#fig-continuity}\n:::\n:::\n",
    "supporting": [
      "chapter5_files/figure-epub"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}