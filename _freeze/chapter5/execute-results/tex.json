{
  "hash": "7f31fd412b6282f5ac475be879b448f8",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n\n\n\n\n\n\n\n\n\n# Likelihood theory\n\n## Finding the MLE numerically\n\nHere's how we simulation $n=100$ random sample from a normal distribution with mean $\\mu=8$ and $\\sigma=1$. \n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- rnorm(n = 100, mean = 8)\nmean(X) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.91364\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe mean is found to be 7.91.\nHere's a plot of the log-likelihood function ($\\mu$ against $\\ell(\\mu)$):\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  x = mean(X) + seq(-1, 1, length = 100)\n) |>\n  rowwise() |>\n  mutate(y = sum(dnorm(X, mean = x, log = TRUE))) |>\n  ggplot(aes(x, y)) +\n  geom_line() +\n  geom_segment(linetype = \"dashed\", x = mean(X), xend = mean(X), y = -Inf,\n               yend = sum(dnorm(unlist(X), mean = mean(X), log = TRUE)),\n               size = 0.4, col = \"gray\") +\n  labs(x = expression(mu), y = expression(l(mu)))\n```\n\n::: {.cell-output-display}\n![Log-likelihood function of the normal mean.](chapter5_files/figure-pdf/fig-mlenumericalplot-1.pdf){#fig-mlenumericalplot fig-pos='H'}\n:::\n:::\n\n\n\n\n\nHere's how to optimise the (log-)likelihood function.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nneg_loglik <- function(theta, data = X) {\n  -1 * sum(dnorm(x = data, mean = theta, log = TRUE))\n}\n\nres <- nlminb(\n  start = 1,  # starting value\n  objective = neg_loglik, \n  control = list(\n    trace = 1  # trace the progress of the optimiser\n  ))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  0:     2535.2413:  1.00000\n  1:     569.78526:  5.00000\n  2:     145.32034:  7.91366\n  3:     145.32034:  7.91364\n  4:     145.32034:  7.91364\n```\n\n\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nglimpse(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 6\n $ par        : num 7.91\n $ objective  : num 145\n $ convergence: int 0\n $ iterations : int 4\n $ evaluations: Named int [1:2] 6 7\n  ..- attr(*, \"names\")= chr [1:2] \"function\" \"gradient\"\n $ message    : chr \"relative convergence (4)\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n## Variance reduction: *Rao-Blackwell*isation\n\nIt is possible to reduce the variance of an unbiased estimator by conditioning on a sufficient statistic.\n\n::: {#thm-raoblackwell}\n### Rao-Blackwell\n\nSuppose that $\\hat\\theta(\\bX)$ is unbiased for $\\theta$, and $S(\\bX)$ is sufficient for $\\theta$.\nThen the function of $S$ defined by\n$$\n\\phi(S) = \\E_\\theta(\\hat\\theta|S) \n$$\n\ni. is a statistic, i.e. $\\phi(S)$ does not involve $\\theta$;\nii. is an unbiased statistic, i.e. $\\E(\\phi(S)) = \\theta$; and\niii. has $\\Var_\\theta(\\phi(S)) \\leq \\Var_\\theta (\\hat\\theta)$, with equality iff $\\hat\\theta$ itself is a function of $S$.\n:::  \n\nIn other words, $\\phi(S)$ is a uniformly \\underline{better} unbiased estimator for $\\theta$.\nThus the Rao-Blackwell theorem provides a systematic method of variance reduction for an estimator that is not a function of the sufficient statistic.\n\n::: {.proof}\n\ni. Since $S$ is sufficient, the distribution of $\\bX$ given $S$ does not involve $\\theta$, and hence $\\E_\\theta(\\hat\\theta(\\bX)|S)$ does not involve $\\theta$.\n\nii. $\\E(\\phi(S)) = \\E\\left[ \\E(\\hat\\theta|S) \\right] = \\E(\\hat\\theta) = \\theta$.\n\niii. Using the law of total variance, \\begin{align*}\n\\Var(\\hat\\theta) \n&= \\E\\left[ \\Var(\\hat\\theta|S) \\right] + \\Var\\left[ \\E(\\hat\\theta|S) \\right] \\\\\n&= \\E\\left[ \\Var(\\hat\\theta|S) \\right] + \\Var(\\phi(S)) \\\\\n&\\geq \\Var(\\phi(S)),\n\\end{align*} with equality iff $\\Var(\\hat\\theta|S) =0$, i.e. iff $\\hat\\theta$ is a function of $S$.\n\n:::\n\n::: {#exm-raoblackwell}\nSuppose we have data $X_1,\\dots,X_n\\iid\\Pois(\\lambda)$ pertaining to the number of road accidents per day, and we want to estimate the probability of having no accidents $\\theta = e^{-\\lambda}=\\Pr(X_i=0)$.\n\n\nAn unbiased estimator of $\\theta$ is \n$$\n\\hat\\theta(\\bX) = \\begin{cases}\n1 & X_1 =0 \\\\\n0 & \\text{otherwise,}\n\\end{cases} \n$$\nas $\\E \\hat\\theta(\\bX) = 1\\cdot\\Pr(X_1=0)=e^{-\\lambda}=\\theta$.\nBut this is likely to be a poor estimator, since it ignores the rest of the sample $X_2,X_3,\\dots,X_n$.\n\nWe can see that $S(\\bX)=\\sum_{i=1}^n X_i$ is sufficient since the joint pdf can be expressed as\n$$\nf(\\bx|\\lambda) = \\frac{1}{x_1!\\cdots x_n!} \\cdot e^{-n\\lambda}\\lambda^{\\sum_{i=1}^nx_i}.\n$$\n\nNow apply the Rao-Blackwell theorem:\n\\begin{align*}\n\\phi(S) = \\E(\\hat\\theta|S) = \\E\\Big(\\hat\\theta \\, \\Big| \\, \\sum_{i=1}^n X_i = S  \\Big) \n&= \\Pr\\Big(X_1=0  \\, \\Big| \\, \\sum_{i=1}^n X_i = S  \\Big)\\\\\n&= \\left(1 - \\frac{1}{n} \\right)^S,\n\\end{align*}\nwhere the conditional probability in the last step comes from the Poisson-binomial relationship \n(Refer Ex. Sheet 2: Suppose $X_i\\iid\\Pois(\\lambda_i)$, then $X_1\\big|(\\sum_{i=1}^nX_i=m)\\sim\\Bin(m,\\pi)$, where $\\pi=\\lambda_1/\\sum_{i=1}^n \\lambda_i$).\n\nBy the Rao-Blackwell theorem, $\\Var(\\phi)<\\Var(\\hat\\theta(\\bX))$ (strict inequality since $\\hat\\theta(\\bX)$ is not a function of $S$), so prefer $\\phi(S)$ over $\\hat\\theta(\\bX)$ as an estimator.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlambda <- 2\nn <- 25\n(theta <- dpois(x = 0, lambda = lambda))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1353353\n```\n\n\n:::\n\n```{.r .cell-code}\nB <- 1000\nX <- matrix(rpois(n * B, lambda = lambda), ncol = B)\ntheta_hat <- apply(X, 2, function(x) as.numeric(x[1] == 0))\nphi_hat <- apply(X, 2, function(x) (1-1/n)^(sum(x)))\n\ntibble(theta_hat, phi_hat) %>%\n  pivot_longer(everything(), names_to = \"Estimator\", values_to = \"theta_hat\") %>%\n  ggplot() +\n  geom_density(aes(theta_hat, col = Estimator, fill = Estimator), alpha = 0.6) +\n  # # geom_vline(xintercept = theta, linetype = \"dashed\") +\n  # geom_vline(data = tibble(\n  #   x = c(mean(MLE), mean(MOM)),\n  #   Estimator = c(\"MLE\", \"MOM\")\n  # ), aes(xintercept = x), linetype = \"dashed\") +\n  facet_grid(. ~ Estimator) +\n  # labs(x = expression(hat(theta)), y = expression(f~(hat(theta)))) +\n  # scale_x_continuous(breaks = seq(2, 14, by = 2)) +\n  theme(legend.position = \"none\")\n  # geom_text(data = tibble(x = c(4.9, 5.85), y = c(0.45, 0.45),\n  #                         Estimator = c(\"MLE\", \"MOM\"),\n  #                         label = c(\"E(hat(theta)[ML])\", \"E(hat(theta)[MOM])\")),\n  #           aes(x, y, label = label), parse = TRUE)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-pdf/fig-raoblackwellisation-1.pdf){#fig-raoblackwellisation fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n\nBut is $\\phi(S)=(1-1/n)^S$ unbiased? This is guaranteed by the RB theorem. Check: Since $S=\\sum_{i=1}^n X_i \\sim\\Pois(n\\lambda)$, we get\n\\begin{align*}\n\\E(\\phi(S)) &= \\sum_{s=0}^\\infty \\left(1 - \\frac{1}{n} \\right)^s  \\frac{e^{-n\\lambda}(n\\lambda)^s}{s!}\\times e^{-\\lambda}e^{\\lambda} \\\\\n&= e^{-\\lambda} \\sum_{s=0}^\\infty \\myunderbrace{\\frac{e^{-\\lambda(n-1)}[\\lambda(n-1)]^s}{s!}}{\\text{pmf of }\\Pois(\\lambda(n-1))} = e^{-\\lambda}.\n\\end{align*}\nA similar calculation can give us the variance of this estimator.\n\n:::\n\n\n## Continuity\n\nA continuous function $\\psi(x)$ is a function such that a continuous variation of $x$ induces a continuous variation of $\\psi(x)$--i.e. no jumps allowed.\n[$\\psi$ is continuous at $c$ if $\\forall \\epsilon > 0$, $\\exists \\delta > 0$ s.t. $|x-c| < \\delta \\Rightarrow |\\psi(x)-\\psi(c)| < \\epsilon$.]{.onslideh}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  x = seq(1, 3, length = 1000),\n  y = 16 - x^2\n) -> plot_df\n\nmycols <- grDevices::palette.colors(3, palette = \"Set1\")\n\nggplot() +\n  annotate(\"rect\", xmin = 2 - 0.25, xmax = 2 + 0.25, ymin = -Inf, ymax = Inf, \n           fill = mycols[1], alpha = 0.3) +\n  annotate(\"rect\", xmin = -Inf, xmax = Inf, ymin = 12 - 2, ymax = 12 + 2, \n           fill = mycols[2], alpha = 0.3)  +\n  geom_line(data = plot_df, aes(x, y)) +\n  geom_line(data = plot_df %>% filter(x >= 2 - 0.25, x <= 2 + 0.25), aes(x, y),\n            col = mycols[3], linewidth = 2) +  \n  # geom_segment(aes(x = 2, xend = 2, y = 12, yend = -Inf), linetype = \"dashed\",\n  #              size = 0.4) \n  geom_hline(yintercept = 12, linetype = \"dashed\") +\n  geom_vline(xintercept = 2, linetype = \"dashed\") +\n  scale_x_continuous(breaks = 2 + c(-0.25, 0, 0.25), \n                     labels = c(expression(\"c-\"*delta),\n                                \"c\",\n                                expression(\"c+\"*delta))) +\n  scale_y_continuous(breaks = 12 + c(-2, 0, 2), \n                     labels = c(expression(\"f(c)-\"*epsilon),\n                                \"f(c)\",\n                                expression(\"f(c)+\"*epsilon)))  \n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-pdf/fig-continuity-1.pdf){#fig-continuity fig-pos='H'}\n:::\n:::\n",
    "supporting": [
      "chapter5_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}