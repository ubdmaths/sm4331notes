{
  "hash": "8ebafb3071439fc2e8e6705c6378e5ca",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n\n\n\n\n# Using `nlminb()` for Maximum Likelihood Estimation\n\nConsider Q2 in the Exercise Sheet 5 (Hypothesis Testing).\nA random sample $X_1,\\dots,X_n$ is drawn from a Pareto distribution with pdf\n$$\nf(x \\mid \\alpha,\\nu) = \\frac{\\alpha\\nu^\\alpha}{x^{\\alpha+1}} \\quad \\text{for } x > \\nu, \\  \\alpha > 0, \\ \\nu > 0\n$$\nThe Pareto distribution is frequently used in economics to model income and wealth distributions, especially the upper tail--where a small fraction of the population holds a disproportionately large share of income or wealth. \nThis fits the famous Pareto Principle or 80/20 rule.\n\nThe two parameters in the Pareto distribution:\n\n-\t$\\nu$ (scale parameter): the minimum possible income/wealth (i.e., the distribution starts at this value),\n- $\\alpha$ (shape parameter): controls the \"fatness\" of the tail, smaller $\\alpha$ means fatter tails and greater inequality. In wealth modelling, this is the so-called *Pareto index*.\n\nAn example from empirical literature (e.g. Atkinson and Piketty, 2007) suggests that $\\alpha$ for income in the U.S. top 1% is around 1.5-2.5, depending on the year and method, while $\\nu$ varies depending on the income bracket analyzed, typically \\$100k to \\$500k for high earners.\n\nHere is what the pdf looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# True values\nalpha <- 2  # shape parameter\nnu <- 250  # thousands of dollars, say\n\ntibble(\n  x = seq(nu, nu*4, length.out = 1000),\n  f = case_when(\n    x < alpha ~ 0,\n    TRUE ~ alpha * nu^alpha / x^(alpha + 1)\n)) |>\n  ggplot(aes(x, f)) +\n  geom_line() +\n  geom_vline(xintercept = nu, linetype = \"dashed\") +\n  annotate(\"segment\", x = nu, xend = nu / 2, y = 0, yend = 0) +\n  annotate(\"text\", x = nu, y = 3e-3, label = expression(nu), hjust = 2) +\n  labs(\n    title = expression(\"Pareto distribution with\"~alpha~\"=\"~2~\"and\"~nu~\"=\"~250),\n    x = \"x\",\n    y = expression(f(x~\"|\"~alpha, nu))\n  ) +\n  scale_x_continuous(labels = scales::dollar) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](nlminb_files/figure-pdf/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe following code generates a random sample of size $n$ from the Pareto distribution with parameters $\\alpha = 2$ and $\\nu = 250$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(VGAM)\nset.seed(1)\n\nn <- 250  # sample size\nX <- rpareto(n, scale = nu, shape = alpha)\nhead(X, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  485.1775  409.8229  330.3074  262.3297  556.6811  263.7592  257.2164\n [8]  307.5429  315.1921 1005.7592\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  250.9   292.4   354.7   428.1   469.4  2186.1 \n```\n\n\n:::\n:::\n\n\nAnd suppose we were to plot a histogram of the sample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data.frame(x = X), aes(x)) +\n  geom_histogram(col = \"white\", binwidth = 50, boundary = nu) +\n  scale_x_continuous(labels = scales::dollar) +\n  coord_cartesian(xlim = c(nu / 2, 1000)) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](nlminb_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n::: {.callout-caution}\n### Think\n\nWhen we \"draw\" samples from a particular pdf, we expect the distribution of the sample (i.e., the histogram) to resemble the theoretical pdf. \nDo you see any resemblance?\nLooking ahead to parameter estimation, suppose the true value of $\\nu$ was not known. \nWhat value would you guess for $\\nu$ based on the data? \n\n:::\n\n\n## Paremeter estimation using MLE\n\nIn class, we solved for the MLE of $\\alpha$ and $\\nu$ in the usual way using derivatives and sketching the likelihood function.\nRecall that\n$$\n\\hat\\alpha = \\frac{n}{\\sum_{i=1}^n \\log(X_i/\\hat\\nu)} \\quad \\text{and} \\quad \n\\hat\\nu = \\min(X_i).\n$$\n\nIf we plug in the data to compute the MLE, we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnu_hat <- min(X)\nalpha_hat <- n / sum(log(X / nu_hat))\ncat(\"nu_hat =\", nu_hat, \"\\nalpha_hat =\", alpha_hat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnu_hat = 250.9195 \nalpha_hat = 2.267916\n```\n\n\n:::\n:::\n\n\nWe can also let the computer do the work for us using the `nlminb()` function in R.\nThis function is a general-purpose optimization function that can be used to find the maximum likelihood estimates of parameters in a statistical model.\nWhat we need is to first code the likelihood function, and then use `nlminb()` to find the values of $\\alpha$ and $\\nu$ that maximize the likelihood function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The pdf function\nfx <- function(x, alpha, nu) {\n  alpha * nu^alpha / x^(alpha + 1)\n}\nfx(X[1:10], alpha, nu)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.0010944805 0.0018160229 0.0034686077 0.0069241716 0.0007245869\n [6] 0.0068121958 0.0073453722 0.0042972722 0.0039919405 0.0001228649\n```\n\n\n:::\n\n```{.r .cell-code}\n# The log-likelihood function\nll <- function(theta) {\n  alpha <- theta[1]\n  nu <- theta[2]\n  \n  # Return really small value if support condition is violated\n  if (alpha <= 0 | nu <= 0 | any(X < nu)) return(-1e10)\n  \n  sum(log(fx(X, alpha, nu)))\n}\nll(theta = c(2, 250))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1540.532\n```\n\n\n:::\n:::\n\n\nHere's a plot of the 2-dimensional log-likelihood function based on the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexpand_grid(\n  nu = seq(230, min(X), length.out = 100),\n  alpha = seq(1, 3, length.out = 100)\n) |>\n  mutate(ll = purrr::map2_dbl(alpha, nu, ~ll(c(.x, .y)))) |>\n  filter(ll > -1e10) |>\n  ggplot(aes(nu, alpha, z = ll)) +\n  geom_raster(aes(fill = ll)) +\n  geom_contour(color = \"white\") +\n  scale_fill_viridis_c() +\n  annotate(\"point\", x = nu, y = alpha, color = \"red3\", size = 2) +\n  annotate(\"text\", x = nu, y = alpha + 0.1, label = \"Truth\", color = \"red3\", vjust = 4) +\n  annotate(\"point\", x = nu_hat, y = alpha_hat, size = 2) +\n  annotate(\"text\", x = nu_hat, y = alpha_hat + 0.1, label = \"MLE\", vjust = 0.5, hjust = 1) +\n  scale_x_continuous(labels = scales::dollar, expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  labs(\n    title = \"Log-likelihood function\",\n    x = expression(nu),\n    y = expression(alpha),\n    fill = \"Log-lik.\\nvalue\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](nlminb_files/figure-pdf/unnamed-chunk-8-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nThe *profile log-likelihood function* \n$$\nf(\\alpha) = \\max_{\\nu} \\ell(\\alpha, \\nu) = \\ell(\\alpha \\mid \\hat\\nu)\n$$\ncan be sketched as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n  alpha = seq(1, 3, length.out = 100),\n  ll = map_dbl(alpha, ~ll(c(.x, nu_hat)))\n) |>\n  ggplot(aes(alpha, ll)) +\n  geom_line(linewidth = 1) +\n  geom_segment(\n    data = tibble(\n      x = c(alpha_hat, alpha_hat),\n      y = c(-Inf, ll(c(alpha_hat, nu_hat))),\n      xend = c(alpha_hat, -Inf),\n      yend = rep(ll(c(alpha_hat, nu_hat)), 2)\n    ),\n    aes(x = x, y = y, xend = xend, yend = yend),\n    linetype = \"dashed\",\n  ) +\n  \n  theme_minimal() +\n  labs(\n    title = \"Profile log-likelihood function\",\n    x = expression(alpha),\n    y = \"Log-likelihood value\"\n  )\n```\n\n::: {.cell-output-display}\n![](nlminb_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nNow, we use `nlminb()` to find the MLE of $\\alpha$ and $\\nu$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- nlminb(\n  start = c(alpha, nu),  # initial \"guess\"\n  objective = function(theta) -1 * ll(theta),  # negative log-likelihood\n  lower = 0,\n  upper = c(Inf, min(X))\n)\nprint(res)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$par\n[1]   2.267916 250.919541\n\n$objective\n[1] 1536.801\n\n$convergence\n[1] 0\n\n$iterations\n[1] 6\n\n$evaluations\nfunction gradient \n       9       16 \n\n$message\n[1] \"both X-convergence and relative convergence (5)\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare nlminb to direct calculations. They are identical!\ncat(\"nu_hat (calculation) =\", nu_hat, \"vs. nu_hat (MLE) =\", res$par[2],  \n    \"\\nalpha_hat (calculation) =\", alpha_hat, \"vs. alpha_hat (MLE) =\", res$par[1], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nnu_hat (calculation) = 250.9195 vs. nu_hat (MLE) = 250.9195 \nalpha_hat (calculation) = 2.267916 vs. alpha_hat (MLE) = 2.267916 \n```\n\n\n:::\n:::\n\n\nWe can also check that the gradients are close to zero at the MLE.\nBut only for the $\\alpha$ parameter, since the log-likelihood is **not differentiable** at $\\nu$!\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gradient at MLE\nnumDeriv::grad(\n  function(theta) -1 * ll(theta), \n  res$par\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.654437e-06 1.937072e+12\n```\n\n\n:::\n:::\n\n\nThe Hessian (observed Fisher information matrix) can also be obtained as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nJ <- -1 * numDeriv::hessian(ll, res$par)\nprint(J)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]          [,2]\n[1,] 48.6055593 -6.093646e-01\n[2,] -0.6093646  1.350050e+09\n```\n\n\n:::\n\n```{.r .cell-code}\nsolve(J)  # To get asymptotic variance\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]         [,2]\n[1,] 2.057378e-02 9.286271e-12\n[2,] 9.286271e-12 7.407132e-10\n```\n\n\n:::\n\n```{.r .cell-code}\n# Standard errors\nse <- sqrt(diag(solve(J)))\nprint(se)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.434356e-01 2.721605e-05\n```\n\n\n:::\n:::\n\n\n",
    "supporting": [
      "nlminb_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}